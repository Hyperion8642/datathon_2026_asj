{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bcf50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import json\n",
    "from folium.plugins import HeatMap\n",
    "from shapely.geometry import MultiPoint, Point\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1795c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Access_to_Everyday_Life_Dataset.csv')\n",
    "df.rename(columns = {'geometry/coordinates/0':'lon','geometry/coordinates/1':'lat', 'properties/attribute_id':'att_id', \n",
    "                     'properties/label_type':'label','properties/neighborhood':'neighborhood','properties/severity':'severity',\n",
    "                    'properties/is_temporary':'is_temp'},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec4af1",
   "metadata": {},
   "source": [
    "## Join geographic data with Everyday Life Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0605534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          lon        lat neighborhood_accurate\n",
      "0 -122.298981  47.594616              Atlantic\n",
      "1 -122.301071  47.593357              Atlantic\n",
      "2 -122.301079  47.596844              Atlantic\n",
      "3 -122.301071  47.596500              Atlantic\n",
      "4 -122.306274  47.599930              Atlantic\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the GeoJSON file\n",
    "neighborhoods_gdf = gpd.read_file('Neighborhood_Map_Atlas_Neighborhoods.geojson')\n",
    "\n",
    "# 3. Convert your DataFrame into a GeoDataFrame\n",
    "# We create a 'geometry' column using the longitude and latitude\n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "incidents_gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "# Set the Coordinate Reference System (CRS)\n",
    "# GeoJSON snippet uses 'CRS84' (EPSG:4326): standard Lat/Lon\n",
    "incidents_gdf.set_crs(epsg=4326, inplace=True)\n",
    "neighborhoods_gdf.to_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# 5. Perform the Spatial Join\n",
    "# This checks which neighborhood polygon each incident point \"intersects\"\n",
    "# We only bring over 'S_HOOD' (Specific Neighborhood) and 'L_HOOD' (Larger Region)\n",
    "joined_df = gpd.sjoin(\n",
    "    incidents_gdf, \n",
    "    neighborhoods_gdf[['S_HOOD', 'L_HOOD', 'geometry']], \n",
    "    how='left', \n",
    "    predicate='intersects'\n",
    ")\n",
    "\n",
    "# 6. Final Clean up\n",
    "# Rename 'S_HOOD' to 'neighborhood_accurate' and remove the geometry column if no longer needed\n",
    "joined_df = joined_df.rename(columns={'S_HOOD': 'neighborhood_accurate'})\n",
    "df_final = pd.DataFrame(joined_df.drop(columns='geometry'))\n",
    "\n",
    "print(df_final[['lon', 'lat', 'neighborhood_accurate']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3ce562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns = ['type','geometry/type'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c469d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>att_id</th>\n",
       "      <th>label</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>severity</th>\n",
       "      <th>is_temp</th>\n",
       "      <th>index_right</th>\n",
       "      <th>neighborhood_accurate</th>\n",
       "      <th>L_HOOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.298981</td>\n",
       "      <td>47.594616</td>\n",
       "      <td>52096165</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Central Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.301071</td>\n",
       "      <td>47.593357</td>\n",
       "      <td>52096166</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Central Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.301079</td>\n",
       "      <td>47.596844</td>\n",
       "      <td>52096167</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Central Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.301071</td>\n",
       "      <td>47.596500</td>\n",
       "      <td>52096168</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Central Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.306274</td>\n",
       "      <td>47.599930</td>\n",
       "      <td>52096365</td>\n",
       "      <td>NoCurbRamp</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>Central Area</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon        lat    att_id           label neighborhood  severity  \\\n",
       "0 -122.298981  47.594616  52096165  SurfaceProblem     Atlantic       4.0   \n",
       "1 -122.301071  47.593357  52096166  SurfaceProblem     Atlantic       3.0   \n",
       "2 -122.301079  47.596844  52096167  SurfaceProblem     Atlantic       4.0   \n",
       "3 -122.301071  47.596500  52096168  SurfaceProblem     Atlantic       4.0   \n",
       "4 -122.306274  47.599930  52096365      NoCurbRamp     Atlantic       4.0   \n",
       "\n",
       "   is_temp  index_right neighborhood_accurate        L_HOOD  \n",
       "0    False         31.0              Atlantic  Central Area  \n",
       "1    False         31.0              Atlantic  Central Area  \n",
       "2    False         31.0              Atlantic  Central Area  \n",
       "3    False         31.0              Atlantic  Central Area  \n",
       "4    False         31.0              Atlantic  Central Area  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32caa926",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "gdf_incidents = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c729516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Shape: (163893, 28)\n",
      "  neighborhood_accurate            NEIGH_NAME  TOTAL_POPULATION\n",
      "0              Atlantic    Council District 3          108012.0\n",
      "0              Atlantic  23rd & Union-Jackson           16724.0\n",
      "1              Atlantic    Council District 3          108012.0\n",
      "1              Atlantic  23rd & Union-Jackson           16724.0\n",
      "2              Atlantic    Council District 3          108012.0\n"
     ]
    }
   ],
   "source": [
    "# 3. Join with Official Neighborhood Atlas\n",
    "neighborhoods_atlas = gpd.read_file('Neighborhood_Map_Atlas_Neighborhoods.geojson').to_crs(\"EPSG:4326\")\n",
    "\n",
    "gdf_with_hoods = gpd.sjoin(\n",
    "    gdf_incidents, \n",
    "    neighborhoods_atlas[['S_HOOD', 'L_HOOD', 'geometry']], \n",
    "    how='left', \n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "# --- CRITICAL FIXES FOR DOUBLE JOINING ---\n",
    "# 1. Rename the neighborhood column\n",
    "gdf_with_hoods = gdf_with_hoods.rename(columns={'S_HOOD': 'neighborhood_accurate'})\n",
    "\n",
    "# 2. Drop the join index so the next sjoin doesn't conflict\n",
    "if 'index_right' in gdf_with_hoods.columns:\n",
    "    gdf_with_hoods = gdf_with_hoods.drop(columns=['index_right'])\n",
    "\n",
    "# 3. Reset the index to ensure it's clean for the next operation\n",
    "gdf_with_hoods = gdf_with_hoods.reset_index(drop=True)\n",
    "# -----------------------------------------\n",
    "\n",
    "# 4. Join with ACS Demographic Data\n",
    "url = \"https://data-seattlecitygis.opendata.arcgis.com/datasets/SeattleCityGIS::seattle-neighborhoods-top-50-american-community-survey-data.geojson\"\n",
    "acs_gdf = gpd.read_file(url)\n",
    "\n",
    "acs_cols = [\n",
    "    'NEIGH_NAME', 'TOTAL_POPULATION', 'TOTAL_HOUSEHOLDS', 'Children_under_5',\n",
    "    'Children_under_18', 'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "    'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT', 'PER_CAPITA_INCOME',\n",
    "    'RENTER_HOUSEHOLDS_PERCENT', 'DETACHED_1_UNIT_PERCENT',\n",
    "    'PUBLIC_TRANSPORTATION_PERCENT', 'POPULATION_DISABILITY_PERC',\n",
    "    'geometry'\n",
    "]\n",
    "acs_gdf_small = acs_gdf[acs_cols].to_crs(gdf_with_hoods.crs)\n",
    "\n",
    "# Perform the second spatial join\n",
    "merged_final_gdf = gpd.sjoin(\n",
    "    gdf_with_hoods,\n",
    "    acs_gdf_small,\n",
    "    how=\"left\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "\n",
    "# 5. Final Cleanup\n",
    "# If you want to keep it as a dataframe for ML models:\n",
    "df_final_clean = pd.DataFrame(merged_final_gdf.drop(columns='geometry'))\n",
    "\n",
    "print(f\"Final Shape: {df_final_clean.shape}\")\n",
    "print(df_final_clean[['neighborhood_accurate', 'NEIGH_NAME', 'TOTAL_POPULATION']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a2c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>geometry/type</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>att_id</th>\n",
       "      <th>label</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>severity</th>\n",
       "      <th>is_temp</th>\n",
       "      <th>neighborhood_accurate</th>\n",
       "      <th>...</th>\n",
       "      <th>Median_Age</th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "      <th>PEOPLE_OF_COLOR_PERCENT</th>\n",
       "      <th>BACHELOR_HIGHER_PERCENT</th>\n",
       "      <th>PER_CAPITA_INCOME</th>\n",
       "      <th>RENTER_HOUSEHOLDS_PERCENT</th>\n",
       "      <th>DETACHED_1_UNIT_PERCENT</th>\n",
       "      <th>PUBLIC_TRANSPORTATION_PERCENT</th>\n",
       "      <th>POPULATION_DISABILITY_PERC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Point</td>\n",
       "      <td>-122.298981</td>\n",
       "      <td>47.594616</td>\n",
       "      <td>52096165</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>...</td>\n",
       "      <td>36.4</td>\n",
       "      <td>56878.0</td>\n",
       "      <td>51134.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>74.6</td>\n",
       "      <td>99655.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Point</td>\n",
       "      <td>-122.298981</td>\n",
       "      <td>47.594616</td>\n",
       "      <td>52096165</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>...</td>\n",
       "      <td>36.3</td>\n",
       "      <td>8580.0</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>51.7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>70278.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>29.8</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Point</td>\n",
       "      <td>-122.301071</td>\n",
       "      <td>47.593357</td>\n",
       "      <td>52096166</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>...</td>\n",
       "      <td>36.4</td>\n",
       "      <td>56878.0</td>\n",
       "      <td>51134.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>74.6</td>\n",
       "      <td>99655.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Point</td>\n",
       "      <td>-122.301071</td>\n",
       "      <td>47.593357</td>\n",
       "      <td>52096166</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>...</td>\n",
       "      <td>36.3</td>\n",
       "      <td>8580.0</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>51.7</td>\n",
       "      <td>61.9</td>\n",
       "      <td>70278.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>29.8</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Point</td>\n",
       "      <td>-122.301079</td>\n",
       "      <td>47.596844</td>\n",
       "      <td>52096167</td>\n",
       "      <td>SurfaceProblem</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Atlantic</td>\n",
       "      <td>...</td>\n",
       "      <td>36.4</td>\n",
       "      <td>56878.0</td>\n",
       "      <td>51134.0</td>\n",
       "      <td>35.9</td>\n",
       "      <td>74.6</td>\n",
       "      <td>99655.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type geometry/type         lon        lat    att_id           label  \\\n",
       "0  Feature         Point -122.298981  47.594616  52096165  SurfaceProblem   \n",
       "0  Feature         Point -122.298981  47.594616  52096165  SurfaceProblem   \n",
       "1  Feature         Point -122.301071  47.593357  52096166  SurfaceProblem   \n",
       "1  Feature         Point -122.301071  47.593357  52096166  SurfaceProblem   \n",
       "2  Feature         Point -122.301079  47.596844  52096167  SurfaceProblem   \n",
       "\n",
       "  neighborhood  severity  is_temp neighborhood_accurate  ... Median_Age  \\\n",
       "0     Atlantic       4.0    False              Atlantic  ...       36.4   \n",
       "0     Atlantic       4.0    False              Atlantic  ...       36.3   \n",
       "1     Atlantic       3.0    False              Atlantic  ...       36.4   \n",
       "1     Atlantic       3.0    False              Atlantic  ...       36.3   \n",
       "2     Atlantic       4.0    False              Atlantic  ...       36.4   \n",
       "\n",
       "      Male   Female  PEOPLE_OF_COLOR_PERCENT  BACHELOR_HIGHER_PERCENT  \\\n",
       "0  56878.0  51134.0                     35.9                     74.6   \n",
       "0   8580.0   8144.0                     51.7                     61.9   \n",
       "1  56878.0  51134.0                     35.9                     74.6   \n",
       "1   8580.0   8144.0                     51.7                     61.9   \n",
       "2  56878.0  51134.0                     35.9                     74.6   \n",
       "\n",
       "   PER_CAPITA_INCOME  RENTER_HOUSEHOLDS_PERCENT  DETACHED_1_UNIT_PERCENT  \\\n",
       "0            99655.0                       65.7                     23.6   \n",
       "0            70278.0                       58.4                     29.8   \n",
       "1            99655.0                       65.7                     23.6   \n",
       "1            70278.0                       58.4                     29.8   \n",
       "2            99655.0                       65.7                     23.6   \n",
       "\n",
       "   PUBLIC_TRANSPORTATION_PERCENT  POPULATION_DISABILITY_PERC  \n",
       "0                           17.1                        12.4  \n",
       "0                           16.3                        17.2  \n",
       "1                           17.1                        12.4  \n",
       "1                           16.3                        17.2  \n",
       "2                           17.1                        12.4  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23858f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'geometry/type', 'lon', 'lat', 'att_id', 'label',\n",
       "       'neighborhood', 'severity', 'is_temp', 'neighborhood_accurate',\n",
       "       'L_HOOD', 'index_right', 'NEIGH_NAME', 'TOTAL_POPULATION',\n",
       "       'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
       "       'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
       "       'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
       "       'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
       "       'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
       "       'POPULATION_DISABILITY_PERC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1237f234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 163893 entries, 0 to 81972\n",
      "Data columns (total 28 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   type                           163893 non-null  object \n",
      " 1   geometry/type                  163893 non-null  object \n",
      " 2   lon                            163893 non-null  float64\n",
      " 3   lat                            163893 non-null  float64\n",
      " 4   att_id                         163893 non-null  int64  \n",
      " 5   label                          163893 non-null  object \n",
      " 6   neighborhood                   163893 non-null  object \n",
      " 7   severity                       159392 non-null  float64\n",
      " 8   is_temp                        163893 non-null  bool   \n",
      " 9   neighborhood_accurate          163840 non-null  object \n",
      " 10  L_HOOD                         163840 non-null  object \n",
      " 11  index_right                    163840 non-null  float64\n",
      " 12  NEIGH_NAME                     163840 non-null  object \n",
      " 13  TOTAL_POPULATION               162927 non-null  float64\n",
      " 14  TOTAL_HOUSEHOLDS               162927 non-null  float64\n",
      " 15  Children_under_5               162927 non-null  float64\n",
      " 16  Children_under_18              162927 non-null  float64\n",
      " 17  Older_Adults_65_over           162927 non-null  float64\n",
      " 18  Median_Age                     162927 non-null  float64\n",
      " 19  Male                           162927 non-null  float64\n",
      " 20  Female                         162927 non-null  float64\n",
      " 21  PEOPLE_OF_COLOR_PERCENT        162927 non-null  float64\n",
      " 22  BACHELOR_HIGHER_PERCENT        162927 non-null  float64\n",
      " 23  PER_CAPITA_INCOME              162927 non-null  float64\n",
      " 24  RENTER_HOUSEHOLDS_PERCENT      162927 non-null  float64\n",
      " 25  DETACHED_1_UNIT_PERCENT        162927 non-null  float64\n",
      " 26  PUBLIC_TRANSPORTATION_PERCENT  162927 non-null  float64\n",
      " 27  POPULATION_DISABILITY_PERC     162927 non-null  float64\n",
      "dtypes: bool(1), float64(19), int64(1), object(7)\n",
      "memory usage: 35.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a87083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5001616908592802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_final)/len(df_final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97606a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04488294191942304"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_1 = df_final_clean[df_final_clean['neighborhood_accurate'] == df_final_clean['NEIGH_NAME']]\n",
    "len(check_1)/len(df_final_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63e4f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Council District 3\n",
       "0    23rd & Union-Jackson\n",
       "1      Council District 3\n",
       "1    23rd & Union-Jackson\n",
       "2      Council District 3\n",
       "Name: NEIGH_NAME, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_clean['NEIGH_NAME'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6dc3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Central Area\n",
       "0        Central Area\n",
       "1        Central Area\n",
       "1        Central Area\n",
       "2        Central Area\n",
       "             ...     \n",
       "81970    Central Area\n",
       "81971    Central Area\n",
       "81971    Central Area\n",
       "81972    Central Area\n",
       "81972    Central Area\n",
       "Name: L_HOOD, Length: 163893, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_clean['L_HOOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97dec9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/58/bxfs02q56pzfmzl9zs71htz00000gn/T/ipykernel_12406/3154352185.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labeled['is_temp'] = df_labeled['is_temp'].astype(int)\n",
      "/var/folders/58/bxfs02q56pzfmzl9zs71htz00000gn/T/ipykernel_12406/3154352185.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labeled.dropna(inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Drop na values\n",
    "df_labeled = df_final_clean.dropna(subset=['severity'])\n",
    "# Pre-process 'is_temp' to be 1 and 0\n",
    "df_labeled['is_temp'] = df_labeled['is_temp'].astype(int)\n",
    "features = [ 'lon', 'lat', 'att_id', 'label','is_temp', 'neighborhood_accurate', 'TOTAL_POPULATION',\n",
    "       'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
    "       'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "       'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
    "       'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
    "       'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
    "       'POPULATION_DISABILITY_PERC']\n",
    "target = 'severity'\n",
    "# Drop all remaining nas - those rows are negligible\n",
    "df_labeled.dropna(inplace = True)\n",
    "X = df_labeled[features]\n",
    "y = df_labeled[target]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe020ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Stratification Key\n",
    "# This combines severity and neighborhood to ensure the split is proportional for both.\n",
    "# We fill NAs in categorical columns temporarily to prevent errors in concatenation.\n",
    "stratify_cols = ['severity', 'neighborhood_accurate']\n",
    "stratify_key = df_labeled[stratify_cols].astype(str).agg('-'.join, axis=1)\n",
    "\n",
    "# Handle cases where a neighborhood/severity combo appears only once \n",
    "# (Stratify requires at least 2 members per group)\n",
    "counts = stratify_key.value_counts()\n",
    "valid_indices = stratify_key.isin(counts[counts > 1].index)\n",
    "\n",
    "X_final = X[valid_indices]\n",
    "y_final = y[valid_indices]\n",
    "stratify_final = stratify_key[valid_indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd5cd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 126773\n",
      "Testing set size: 31694\n"
     ]
    }
   ],
   "source": [
    "# 4. Perform the Split (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, \n",
    "    y_final, \n",
    "    test_size=0.20, \n",
    "    random_state=42, \n",
    "    stratify=stratify_final # This keeps neighborhood and severity proportions identical\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "191a65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Re-run the split logic from the previous step \n",
    "# (Assuming X_train, X_test, y_train, y_test are defined as per the previous stratified split)\n",
    "\n",
    "# 3. Define the ColumnTransformer\n",
    "# This will OneHotEncode 'label' and 'neighborhood_accurate'\n",
    "# 'passthrough' means 'is_temp' will be kept as is (1s and 0s)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee9c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test Set: 72.06%\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a Pipeline\n",
    "# A pipeline bundles the preprocessing and the model together.\n",
    "# This prevents data leakage and makes it easier to predict on new data.\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 5. Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(f\"Model Accuracy on Test Set: {clf.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a50a13",
   "metadata": {},
   "source": [
    "## XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce282642",
   "metadata": {},
   "source": [
    "can't have labels starting from 1.0: use labelencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "856582ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled2 = df_final_clean.dropna(subset=['severity']).copy()\n",
    "df_labeled2['is_temp'] = df_labeled2['is_temp'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a37b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.72      7056\n",
      "         2.0       0.58      0.40      0.47      5844\n",
      "         3.0       0.61      0.75      0.67      9281\n",
      "         4.0       0.81      0.52      0.63      5582\n",
      "         5.0       0.73      0.65      0.69      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.67      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_labeled2['severity']) \n",
    "\n",
    "# 3. Define Features\n",
    "# One-hot encoding neighborhood and label; keeping is_temp as binary\n",
    "features = [ 'lon', 'lat', 'att_id', 'label','is_temp', 'neighborhood_accurate', 'TOTAL_POPULATION',\n",
    "       'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
    "       'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "       'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
    "       'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
    "       'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
    "       'POPULATION_DISABILITY_PERC']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f949bfd",
   "metadata": {},
   "source": [
    "### Running several XGBoost Models, with parameters features. \n",
    "Implement feature subset selection loop - begin with 0 features, iteratively add features that increase Weighted F1-score the most until target limit is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b36caffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full list of candidate features from before\n",
    "all_features = [\n",
    "    'lon', 'lat', 'label', 'is_temp', 'neighborhood_accurate', \n",
    "    'TOTAL_POPULATION', 'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
    "    'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "    'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
    "    'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
    "    'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
    "    'POPULATION_DISABILITY_PERC'\n",
    "]\n",
    "\n",
    "# Helper function to train and score a specific subset\n",
    "def evaluate_subset(feature_subset):\n",
    "    # Identify columns in this subset that need OneHotEncoding\n",
    "    cat_cols = [f for f in feature_subset if f in ['label', 'neighborhood_accurate']]\n",
    "    \n",
    "    # Update preprocessor for only the categorical features present in this subset\n",
    "    current_preprocessor = ColumnTransformer(\n",
    "        transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    current_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', current_preprocessor),\n",
    "        ('classifier', XGBClassifier(n_estimators=50, max_depth=5, learning_rate=0.1, \n",
    "                                     objective='multi:softprob', num_class=5, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    current_pipeline.fit(X_train[feature_subset], y_train)\n",
    "    preds = current_pipeline.predict(X_test[feature_subset])\n",
    "    return f1_score(y_test, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73519b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Optimization...\n",
      "Features: 1 | Best Weighted F1: 0.3982 | Added: neighborhood_accurate\n",
      "Features: 2 | Best Weighted F1: 0.5818 | Added: label\n",
      "Features: 3 | Best Weighted F1: 0.5983 | Added: lat\n",
      "Features: 4 | Best Weighted F1: 0.6054 | Added: lon\n",
      "Features: 5 | Best Weighted F1: 0.6080 | Added: PEOPLE_OF_COLOR_PERCENT\n",
      "Features: 6 | Best Weighted F1: 0.6089 | Added: POPULATION_DISABILITY_PERC\n",
      "Stopping early: No further improvement.\n",
      "\n",
      "--- OPTIMIZATION COMPLETE ---\n",
      "Optimal Number of Features: 6\n",
      "Best Features: ['neighborhood_accurate', 'label', 'lat', 'lon', 'PEOPLE_OF_COLOR_PERCENT', 'POPULATION_DISABILITY_PERC']\n"
     ]
    }
   ],
   "source": [
    "# 2. Sequential Forward Selection Loop\n",
    "best_features = []\n",
    "remaining_features = all_features.copy()\n",
    "best_score = 0\n",
    "history = []\n",
    "\n",
    "print(\"Starting Feature Optimization...\")\n",
    "\n",
    "# We will try to find the best subset up to N-1 features\n",
    "for i in range(len(all_features) - 1):\n",
    "    scores_in_this_round = []\n",
    "    \n",
    "    for feature in remaining_features:\n",
    "        trial_features = best_features + [feature]\n",
    "        score = evaluate_subset(trial_features)\n",
    "        scores_in_this_round.append((score, feature))\n",
    "    \n",
    "    # Find the feature that gave the best improvement\n",
    "    scores_in_this_round.sort(reverse=True)\n",
    "    current_best_score, current_best_feature = scores_in_this_round[0]\n",
    "    \n",
    "    if current_best_score > best_score:\n",
    "        best_score = current_best_score\n",
    "        best_features.append(current_best_feature)\n",
    "        remaining_features.remove(current_best_feature)\n",
    "        history.append((len(best_features), best_score, best_features.copy()))\n",
    "        print(f\"Features: {len(best_features)} | Best Weighted F1: {best_score:.4f} | Added: {current_best_feature}\")\n",
    "    else:\n",
    "        # If adding more features doesn't help, we stop early\n",
    "        print(\"Stopping early: No further improvement.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
    "print(f\"Optimal Number of Features: {len(best_features)}\")\n",
    "print(f\"Best Features: {best_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1628debd",
   "metadata": {},
   "source": [
    "Program initially stopped early (after 5 features) - realized model is likely memoorizing features due to att_id. Geographic features may also limit impact of predictions. Initially removed \"att_id\" after first run-through.\n",
    "\n",
    "Features after second run-through: \n",
    "Best Features: ['neighborhood_accurate', 'label', 'lat', 'lon', 'PEOPLE_OF_COLOR_PERCENT', 'POPULATION_DISABILITY_PERC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e62da2",
   "metadata": {},
   "source": [
    "Using features identified after second run-through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07fe58c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.73      7056\n",
      "         2.0       0.59      0.39      0.47      5844\n",
      "         3.0       0.61      0.75      0.67      9281\n",
      "         4.0       0.80      0.53      0.64      5582\n",
      "         5.0       0.74      0.64      0.69      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.68      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'lat', 'lon', 'PEOPLE_OF_COLOR_PERCENT', 'POPULATION_DISABILITY_PERC']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520235bb",
   "metadata": {},
   "source": [
    "Running the code used above to evaluate models with different parameters, but seeing if there are better factors than lat and lon - XGBoost model may have started memorizing those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0eaa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full list of candidate features from before\n",
    "all_features = [\n",
    "    'label', 'is_temp', 'neighborhood_accurate', \n",
    "    'TOTAL_POPULATION', 'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
    "    'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "    'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
    "    'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
    "    'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
    "    'POPULATION_DISABILITY_PERC'\n",
    "]\n",
    "\n",
    "# Helper function to train and score a specific subset\n",
    "def evaluate_subset(feature_subset):\n",
    "    # Identify columns in this subset that need OneHotEncoding\n",
    "    cat_cols = [f for f in feature_subset if f in ['label', 'neighborhood_accurate']]\n",
    "    \n",
    "    # Update preprocessor for only the categorical features present in this subset\n",
    "    current_preprocessor = ColumnTransformer(\n",
    "        transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    current_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', current_preprocessor),\n",
    "        ('classifier', XGBClassifier(n_estimators=50, max_depth=5, learning_rate=0.1, \n",
    "                                     objective='multi:softprob', num_class=5, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    current_pipeline.fit(X_train[feature_subset], y_train)\n",
    "    preds = current_pipeline.predict(X_test[feature_subset])\n",
    "    return f1_score(y_test, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62920431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure you are using the correct dataframe that has ALL the columns\n",
    "# (Using df_final_clean or your most recent merged dataframe)\n",
    "df_working = df_final_clean.dropna(subset=['severity']).copy()\n",
    "\n",
    "# 2. Make sure is_temp is an integer (XGBoost requirement)\n",
    "if 'is_temp' in df_working.columns:\n",
    "    df_working['is_temp'] = df_working['is_temp'].astype(int)\n",
    "\n",
    "# 3. Re-define X and y with the full candidate list\n",
    "X = df_working[all_features]\n",
    "y = LabelEncoder().fit_transform(df_working['severity'])\n",
    "\n",
    "# 4. Re-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# NOW you can run your \"Starting Feature Optimization...\" loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87149dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Optimization...\n",
      "Features: 1 | Best Weighted F1: 0.3982 | Added: neighborhood_accurate\n",
      "Features: 2 | Best Weighted F1: 0.5818 | Added: label\n",
      "Features: 3 | Best Weighted F1: 0.5916 | Added: RENTER_HOUSEHOLDS_PERCENT\n",
      "Features: 4 | Best Weighted F1: 0.5925 | Added: PUBLIC_TRANSPORTATION_PERCENT\n",
      "Features: 5 | Best Weighted F1: 0.5927 | Added: Older_Adults_65_over\n",
      "Features: 6 | Best Weighted F1: 0.5933 | Added: TOTAL_POPULATION\n",
      "Features: 7 | Best Weighted F1: 0.5933 | Added: Male\n",
      "Stopping early: No further improvement.\n",
      "\n",
      "--- OPTIMIZATION COMPLETE ---\n",
      "Optimal Number of Features: 7\n",
      "Best Features: ['neighborhood_accurate', 'label', 'RENTER_HOUSEHOLDS_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT', 'Older_Adults_65_over', 'TOTAL_POPULATION', 'Male']\n"
     ]
    }
   ],
   "source": [
    "# 2. Sequential Forward Selection Loop\n",
    "best_features = []\n",
    "remaining_features = all_features.copy()\n",
    "best_score = 0\n",
    "history = []\n",
    "\n",
    "print(\"Starting Feature Optimization...\")\n",
    "\n",
    "# We will try to find the best subset up to N-1 features\n",
    "for i in range(len(all_features) - 1):\n",
    "    scores_in_this_round = []\n",
    "    \n",
    "    for feature in remaining_features:\n",
    "        trial_features = best_features + [feature]\n",
    "        score = evaluate_subset(trial_features)\n",
    "        scores_in_this_round.append((score, feature))\n",
    "    \n",
    "    # Find the feature that gave the best improvement\n",
    "    scores_in_this_round.sort(reverse=True)\n",
    "    current_best_score, current_best_feature = scores_in_this_round[0]\n",
    "    \n",
    "    if current_best_score > best_score:\n",
    "        best_score = current_best_score\n",
    "        best_features.append(current_best_feature)\n",
    "        remaining_features.remove(current_best_feature)\n",
    "        history.append((len(best_features), best_score, best_features.copy()))\n",
    "        print(f\"Features: {len(best_features)} | Best Weighted F1: {best_score:.4f} | Added: {current_best_feature}\")\n",
    "    else:\n",
    "        # If adding more features doesn't help, we stop early\n",
    "        print(\"Stopping early: No further improvement.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- OPTIMIZATION COMPLETE ---\")\n",
    "print(f\"Optimal Number of Features: {len(best_features)}\")\n",
    "print(f\"Best Features: {best_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3a832",
   "metadata": {},
   "source": [
    "Using features from analysis used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73529766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.60      0.84      0.70      7056\n",
      "         2.0       0.57      0.31      0.41      5844\n",
      "         3.0       0.59      0.74      0.66      9281\n",
      "         4.0       0.81      0.48      0.60      5582\n",
      "         5.0       0.67      0.65      0.66      4116\n",
      "\n",
      "    accuracy                           0.63     31879\n",
      "   macro avg       0.65      0.60      0.61     31879\n",
      "weighted avg       0.64      0.63      0.61     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'RENTER_HOUSEHOLDS_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT', 'Older_Adults_65_over', 'TOTAL_POPULATION', 'Male']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9899d",
   "metadata": {},
   "source": [
    "Created following models based on intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f72cc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.73      7056\n",
      "         2.0       0.60      0.39      0.48      5844\n",
      "         3.0       0.61      0.75      0.67      9281\n",
      "         4.0       0.80      0.53      0.63      5582\n",
      "         5.0       0.73      0.64      0.69      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.68      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'lat', 'lon', 'TOTAL_POPULATION', 'is_temp']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7068b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.72      7056\n",
      "         2.0       0.60      0.39      0.47      5844\n",
      "         3.0       0.61      0.75      0.67      9281\n",
      "         4.0       0.80      0.53      0.63      5582\n",
      "         5.0       0.73      0.64      0.68      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.67      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'lat', 'lon', 'is_temp']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "962ba4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.73      7056\n",
      "         2.0       0.60      0.39      0.47      5844\n",
      "         3.0       0.61      0.76      0.68      9281\n",
      "         4.0       0.80      0.53      0.64      5582\n",
      "         5.0       0.74      0.65      0.69      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.68      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'lat', 'lon', 'is_temp','BACHELOR_HIGHER_PERCENT','PER_CAPITA_INCOME',\n",
    "            'RENTER_HOUSEHOLDS_PERCENT','PUBLIC_TRANSPORTATION_PERCENT','POPULATION_DISABILITY_PERC','Older_Adults_65_over']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1615cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = [\n",
    "    'label', 'is_temp', 'neighborhood_accurate', \n",
    "    'TOTAL_POPULATION', 'TOTAL_HOUSEHOLDS', 'Children_under_5', 'Children_under_18',\n",
    "    'Older_Adults_65_over', 'Median_Age', 'Male', 'Female',\n",
    "    'PEOPLE_OF_COLOR_PERCENT', 'BACHELOR_HIGHER_PERCENT',\n",
    "    'PER_CAPITA_INCOME', 'RENTER_HOUSEHOLDS_PERCENT',\n",
    "    'DETACHED_1_UNIT_PERCENT', 'PUBLIC_TRANSPORTATION_PERCENT',\n",
    "    'POPULATION_DISABILITY_PERC'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19dafa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.84      0.72      7056\n",
      "         2.0       0.60      0.39      0.47      5844\n",
      "         3.0       0.61      0.75      0.67      9281\n",
      "         4.0       0.80      0.53      0.63      5582\n",
      "         5.0       0.73      0.64      0.68      4116\n",
      "\n",
      "    accuracy                           0.65     31879\n",
      "   macro avg       0.67      0.63      0.64     31879\n",
      "weighted avg       0.66      0.65      0.64     31879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['neighborhood_accurate', 'label', 'lat', 'lon', 'is_temp']\n",
    "\n",
    "X = df_labeled2[features].copy()\n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['label', 'neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85849ba4",
   "metadata": {},
   "source": [
    "### Trying original non-joined dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eae1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying original non-joined dataset\n",
    "\n",
    "df_final.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0441e438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lon                      0\n",
       "lat                      0\n",
       "att_id                   0\n",
       "label                    0\n",
       "neighborhood             0\n",
       "severity                 0\n",
       "is_temp                  0\n",
       "index_right              0\n",
       "neighborhood_accurate    0\n",
       "L_HOOD                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00cb9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.46      0.53      0.49      3528\n",
      "         2.0       0.40      0.19      0.26      2920\n",
      "         3.0       0.45      0.66      0.53      4640\n",
      "         4.0       0.68      0.43      0.52      2790\n",
      "         5.0       0.56      0.53      0.55      2057\n",
      "\n",
      "    accuracy                           0.49     15935\n",
      "   macro avg       0.51      0.47      0.47     15935\n",
      "weighted avg       0.50      0.49      0.47     15935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_features_og = [\n",
    "    'lat','lon','is_temp', 'neighborhood_accurate'\n",
    "]\n",
    "\n",
    "X = df_final[all_features_og].copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_final['severity']) \n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e6a3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Original Severity Scales):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.46      0.53      0.49      3528\n",
      "         2.0       0.40      0.19      0.26      2920\n",
      "         3.0       0.45      0.66      0.53      4640\n",
      "         4.0       0.68      0.43      0.52      2790\n",
      "         5.0       0.56      0.53      0.55      2057\n",
      "\n",
      "    accuracy                           0.49     15935\n",
      "   macro avg       0.51      0.47      0.47     15935\n",
      "weighted avg       0.50      0.49      0.47     15935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subset_features_og = [\n",
    "   'neighborhood_accurate'\n",
    "]\n",
    "\n",
    "X = df_final[all_features_og].copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_final['severity']) \n",
    "# X['is_temp'] = X['is_temp'].astype(int)\n",
    "\n",
    "# 4. Stratified Split (Ensures 1.0-5.0 distribution is same in train and test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. Pipeline with Preprocessing and XGBoost\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['neighborhood_accurate'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective='multi:softprob', # Predicts probability for each of the 5 buckets\n",
    "        num_class=5,                # Explicitly setting 5 classes\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 6. Train the Model\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "\n",
    "# Convert back to original 1.0-5.0 scale for the report\n",
    "print(\"Classification Report (Original Severity Scales):\")\n",
    "print(classification_report(le.inverse_transform(y_test), le.inverse_transform(y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
